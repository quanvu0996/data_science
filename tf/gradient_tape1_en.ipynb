{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanvu0996/data_science/blob/main/tf/gradient_tape1_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient descent with tensorflow\n",
        "The tools for building machine learning and deep learning models such as Tensorflow and Pytorch are more and more popular. Building a model becomes easy with just one fit statement. However, to optimize or create new architecture, we need a more flexible tool like Gradient Tape to train special architectural models."
      ],
      "metadata": {
        "id": "ug_pOeYnUFHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wHwaYXU-XdB6",
        "outputId": "9108f6bf-8f98-4134-9a8b-95d5edd13d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic concepts of tensorflow\n",
        "**Constant**: is a fixed value tensor. It may be a number (scalar, one-dim tensor), or the matrix (2-dim tensor), or n-dim tensor"
      ],
      "metadata": {
        "id": "5VtIx_8hW5cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant(1)\n",
        "b = tf.constant([3, 5])\n",
        "c = tf.ones(shape=(2,2))\n",
        "\n",
        "print(a, b, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z16zz0meW9h8",
        "outputId": "1271f68a-7f08-4f49-dc50-74bd6ee7f916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor([3 5], shape=(2,), dtype=int32) tf.Tensor(\n",
            "[[1. 1.]\n",
            " [1. 1.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variable**: an object that can change the value."
      ],
      "metadata": {
        "id": "vjzc3FKFYcAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.Variable(initial_value= tf.zeros((2, 2)), trainable= True)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-mboXvHUJBQ",
        "outputId": "8046d0d2-bb4b-4553-c777-8b559c1ca4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
              "array([[0., 0.],\n",
              "       [0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These concepts correspond to the concepts in linear algebra. \n",
        "<br>\n",
        "**Gradient tape** is the derivative tool of Tensorflow. The following syntax calculates the derivative of the function $y=x^2$ where $x=4$, $(\\frac{δy}{δx| x=4} = 8)$:"
      ],
      "metadata": {
        "id": "DuwGSaiKnfFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(4.0)\n",
        " \n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x) # while x is not a tf.Variable => we need to set the tape watch it, or else the derivative will be 0.\n",
        "                  # if x is a tf.Variable then watch is no need\n",
        "    y = x ** 2\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx)\n",
        "del tape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFZOOrlxnvD1",
        "outputId": "e73dde4f-8131-4068-de09-e9e4d4b1667a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(8.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy second-order derivative by the following syntax:"
      ],
      "metadata": {
        "id": "lVcDQq_6qCfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(4.0)\n",
        "\n",
        "with tf.GradientTape() as t2:\n",
        "    t2.watch(x)\n",
        "    with tf.GradientTape() as t:\n",
        "        t.watch(x)\n",
        "        y = x ** 2\n",
        "    dy_dx = t.gradient(y, x)\n",
        "d2y_dx = t2.gradient(dy_dx, x)\n",
        "print(d2y_dx)\n",
        "\n",
        "del t, t2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiGj-7x6qB7M",
        "outputId": "9cbb01bc-91fb-4ae4-921c-d4908ce41bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(2.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple gradient descent pipeline"
      ],
      "metadata": {
        "id": "cTckZjALp6Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will consider an example of building a simple linear regression model:\n",
        "$Y = aX + b$ <br>\n",
        "\n"
      ],
      "metadata": {
        "id": "qN3ij3U5ZPFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.constant([1., 4., 6, 3, 3, 4, 5, 6, 7])\n",
        "Y = tf.constant([0.25, 1.2, 0.79, 0.52, 1.6, 1.7, 1.9, 2, 2])"
      ],
      "metadata": {
        "id": "een1tXwqeNVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, a and b are the parameters of the model, we expect to find optimal values for them so the model could generalize the relationship between $X$ and $Y$. So, $a$ and $b$ in this example are 2 variables.  We make declarations\n"
      ],
      "metadata": {
        "id": "xypx1uaQeN1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.Variable(initial_value=0., trainable= True)\n",
        "b = tf.Variable(initial_value= 0., trainable= True)"
      ],
      "metadata": {
        "id": "UItvT9yqYpyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X and Y represent the real training data, so they are known constants. Their shapes depend on the batch size.<br>\n",
        "We define a simple loss function:"
      ],
      "metadata": {
        "id": "K_sZE1sVbYk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "    return tf.reduce_sum(tf.square(y_true-y_pred))"
      ],
      "metadata": {
        "id": "5RUxbLbMazwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For model training, we execute the gradient descent steps: find prediction value, calculate loss value, calculate partial derivative of the loss function by each parameter, update values of the parameters. The syntax with Gradient Tape  is as following:"
      ],
      "metadata": {
        "id": "LhlWqk1Oe3Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        " \n",
        "with tf.GradientTape(persistent =True) as tape:\n",
        "    # Find prediction value and calculate loss value\n",
        "    y_pred = a*X+b\n",
        "    loss = loss_fn(Y, y_pred)\n",
        " \n",
        "# Calculate partial derivative by each parameter\n",
        "a_gradient = tape.gradient(loss, a)\n",
        "b_gradient = tape.gradient(loss, b)\n",
        " \n",
        "# update value of each parameter: w1 = w0 - learning_rate * d(loss)/dw\n",
        "a.assign_sub(a_gradient*learning_rate)\n",
        "b.assign_sub(b_gradient*learning_rate)\n",
        " \n",
        "print(a)\n",
        "print(b)\n",
        " \n",
        "del tape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGlI-w9-brr8",
        "outputId": "3d3b51ab-ccb2-498e-d459-36120009b7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.116900004>\n",
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.023920001>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set persistent=True to allow tape to calculate gradient(tape.gradient) more than one time. <br>\n",
        "For monitoring the optimal process, we train with many epoch and check the value of the loss function:\n"
      ],
      "metadata": {
        "id": "X-sEZmoJiHFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(x_true, y_true):\n",
        "    with tf.GradientTape(persistent =True) as tape:\n",
        "        # Find prediction value and calculate loss value\n",
        "        y_pred = a*X+b\n",
        "        loss = loss_fn(Y, y_pred)\n",
        "        print(\"Loss: \", loss.numpy())\n",
        " \n",
        "    # calculate partial gradient by each parameter\n",
        "    a_gradient = tape.gradient(loss, a)\n",
        "    b_gradient = tape.gradient(loss, b)\n",
        " \n",
        "    # update value of each parameter: w1 = w0 - learning_rate * d(loss)/dw\n",
        "    a.assign_sub(a_gradient*learning_rate)\n",
        "    b.assign_sub(b_gradient*learning_rate)\n",
        "    print(\"model: Y= %s X + %s\"%(a.numpy(), b.numpy()))\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"Epoch: \", i)\n",
        "    train_step(X, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12O7157viGDL",
        "outputId": "2f048cef-ce7f-446d-ef32-67b73a64fc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "Loss:  8.134604\n",
            "model: Y= 0.18587564 X + 0.038291242\n",
            "Epoch:  1\n",
            "Loss:  4.186867\n",
            "model: Y= 0.22655392 X + 0.0470237\n",
            "Epoch:  2\n",
            "Loss:  2.8102624\n",
            "model: Y= 0.25052384 X + 0.052426066\n",
            "Epoch:  3\n",
            "Loss:  2.330071\n",
            "model: Y= 0.2646282 X + 0.055861536\n",
            "Epoch:  4\n",
            "Loss:  2.1624107\n",
            "model: Y= 0.2729075 X + 0.05813503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the value of loss function decreases over time and value a and b gradually converging becomes stable through epochs. <br>\n",
        "The above functions use the basic gradient descent, in fact we will want to use the more effective optimizers. In addition, the number of parameters of a deep learning network can reach millions of parameters in reality. We do not want to have to update each parameter manually. At that time, we will do the following:\n"
      ],
      "metadata": {
        "id": "Akh-LUpnkFIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "                           tf.keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer='zeros')\n",
        "])\n",
        "\n",
        "optimizer= tf.optimizers.Adam(learning_rate = .07)\n",
        "\n",
        "def train_step2(x_true, y_true):\n",
        "    with tf.GradientTape(persistent =True) as tape:\n",
        "        # Find prediction value and calculate loss value\n",
        "        y_pred = model(tf.expand_dims(X,-1))\n",
        "        loss = loss_fn( tf.expand_dims(Y, -1), y_pred)\n",
        "        print(\"Loss: \", loss.numpy())\n",
        " \n",
        "    # calculate partial gradient by each parameter\n",
        "    variables = model.trainable_variables \n",
        "    gradients = tape.gradient(loss, variables)\n",
        " \n",
        "    # update value of each parameter: w1 = w0 - learning_rate * d(loss)/dw\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    \n",
        "for i in range(5):\n",
        "    print(\"Epoch: \", i)\n",
        "    train_step2(X, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1P8n2daGzrH",
        "outputId": "ce38b8ef-e59a-4ac6-8831-befc67ddbe6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "Loss:  19.457\n",
            "Epoch:  1\n",
            "Loss:  10.9911995\n",
            "Epoch:  2\n",
            "Loss:  5.402389\n",
            "Epoch:  3\n",
            "Loss:  2.5634406\n",
            "Epoch:  4\n",
            "Loss:  2.0542033\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}